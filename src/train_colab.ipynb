{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# INSTALA√á√ÉO\n",
    "# =========================================================\n",
    "!pip install ultralytics pillow\n",
    "\n",
    "print(\"‚úÖ Depend√™ncias instaladas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import yaml\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "# =========================================================\n",
    "# 0. Mount Google Drive\n",
    "# =========================================================\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/colab\"\n",
    "PROJECT_NAME = \"cloud-arch-security-mvp\"\n",
    "\n",
    "DRIVE_PROJECT = f\"{DRIVE_ROOT}/{PROJECT_NAME}\"\n",
    "DRIVE_CHECKPOINTS = f\"{DRIVE_PROJECT}/checkpoints\"\n",
    "DRIVE_DATASET_CACHE = f\"{DRIVE_PROJECT}/kaggle_dataset_cache\"\n",
    "\n",
    "CONTENT_PROJECT = \"/content/yolo-project\"\n",
    "\n",
    "os.makedirs(DRIVE_CHECKPOINTS, exist_ok=True)\n",
    "\n",
    "# =========================================================\n",
    "# 1. Carregar Dataset do Drive\n",
    "# =========================================================\n",
    "print(\"üì• Carregando dataset do Google Drive...\")\n",
    "\n",
    "# Limpa ambiente anterior\n",
    "if Path(CONTENT_PROJECT).exists():\n",
    "    shutil.rmtree(CONTENT_PROJECT)\n",
    "os.makedirs(CONTENT_PROJECT, exist_ok=True)\n",
    "\n",
    "RAW_DATA_PATH = f\"{CONTENT_PROJECT}/raw_data\"\n",
    "\n",
    "# Verifica se o dataset existe no Drive\n",
    "if not os.path.exists(DRIVE_DATASET_CACHE):\n",
    "    print(\"‚ùå Dataset n√£o encontrado!\")\n",
    "    print(f\"   Esperado em: {DRIVE_DATASET_CACHE}\")\n",
    "    print(\"   Fa√ßa upload do dataset para essa pasta no Google Drive\")\n",
    "    raise Exception(\"Dataset n√£o encontrado no Drive\")\n",
    "\n",
    "# Conta arquivos no cache\n",
    "cache_files = os.listdir(DRIVE_DATASET_CACHE)\n",
    "print(f\"   üìÇ Encontrados {len(cache_files)} itens no cache\")\n",
    "\n",
    "# Copia para o ambiente de trabalho\n",
    "print(\"   üìã Copiando para ambiente de trabalho...\")\n",
    "shutil.copytree(DRIVE_DATASET_CACHE, RAW_DATA_PATH, dirs_exist_ok=True)\n",
    "print(f\"‚úÖ Dataset carregado! ({len(os.listdir(RAW_DATA_PATH))} arquivos)\")\n",
    "\n",
    "os.chdir(CONTENT_PROJECT)\n",
    "\n",
    "# =========================================================\n",
    "# 2. Converter Pascal VOC (XML) para YOLO Format\n",
    "# =========================================================\n",
    "print(\"\\nüîÑ Convertendo Pascal VOC para YOLO format...\")\n",
    "\n",
    "RAW_DATA = Path(\"raw_data\")\n",
    "\n",
    "# Coleta todas as classes do dataset\n",
    "all_classes = set()\n",
    "xml_files = list(RAW_DATA.glob(\"**/*.xml\"))  # Busca recursiva\n",
    "print(f\"   Encontrados {len(xml_files)} arquivos XML\")\n",
    "\n",
    "if len(xml_files) == 0:\n",
    "    print(\"‚ùå Nenhum arquivo XML encontrado!\")\n",
    "    print(f\"   Verificando conte√∫do de {RAW_DATA}:\")\n",
    "    for i, item in enumerate(RAW_DATA.iterdir()):\n",
    "        print(f\"      - {item.name}\")\n",
    "        if i > 20:\n",
    "            print(\"      ... (mais arquivos)\")\n",
    "            break\n",
    "    raise Exception(\"Dataset inv√°lido - sem arquivos XML\")\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for obj in root.findall('object'):\n",
    "            class_name = obj.find('name').text\n",
    "            all_classes.add(class_name)\n",
    "    except Exception as e:\n",
    "        pass  # Ignora erros silenciosamente\n",
    "\n",
    "all_classes = sorted(list(all_classes))\n",
    "print(f\"   Total de classes encontradas: {len(all_classes)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. Mapear para 14 Categorias STRIDE\n",
    "# =========================================================\n",
    "print(\"\\nüìä Mapeando para categorias STRIDE...\")\n",
    "\n",
    "CATEGORY_MAPPING = {\n",
    "    'compute': ['EC2', 'Lambda', 'EKS', 'Fargate', 'Container', 'ECS', \n",
    "                'App Service', 'Virtual Machine', 'VM', 'Compute Engine',\n",
    "                'Cloud Run', 'App Engine', 'GKE', 'AKS', 'Kubernetes',\n",
    "                'Elastic Beanstalk', 'Batch', 'Lightsail', 'EMR'],\n",
    "    \n",
    "    'database': ['RDS', 'DynamoDB', 'Aurora', 'DocumentDB', 'ElastiCache',\n",
    "                 'Cosmos DB', 'SQL Database', 'Cloud SQL', 'Firestore',\n",
    "                 'BigQuery', 'Redshift', 'Neptune', 'Cloud Spanner',\n",
    "                 'Managed Database', 'Database', 'DB', 'Redis', 'Memcached'],\n",
    "    \n",
    "    'storage': ['S3', 'EBS', 'EFS', 'Glacier', 'Storage', 'Blob Storage',\n",
    "                'Cloud Storage', 'File Storage', 'Azure Storage', 'GCS',\n",
    "                'Backup', 'Archive', 'Data Lake'],\n",
    "    \n",
    "    'network': ['VPC', 'Virtual Network', 'VNet', 'Subnet', 'Gateway',\n",
    "                'Load Balancer', 'ALB', 'NLB', 'ELB', 'CloudFront',\n",
    "                'CDN', 'Route 53', 'DNS', 'VPN', 'Direct Connect',\n",
    "                'ExpressRoute', 'Cloud Interconnect', 'NAT', 'Firewall',\n",
    "                'Network', 'Internet Gateway', 'Transit Gateway'],\n",
    "    \n",
    "    'security': ['IAM', 'Identity', 'Cognito', 'WAF', 'Shield', 'GuardDuty',\n",
    "                 'Security Hub', 'Key Vault', 'KMS', 'Secrets Manager',\n",
    "                 'Certificate', 'Azure AD', 'Cloud Identity', 'SSO'],\n",
    "    \n",
    "    'api_gateway': ['API Gateway', 'API Management', 'Apigee', 'AppSync',\n",
    "                    'API', 'Gateway', 'Endpoints'],\n",
    "    \n",
    "    'messaging': ['SQS', 'SNS', 'EventBridge', 'Service Bus', 'Pub/Sub',\n",
    "                  'Kinesis', 'Event Hub', 'MQ', 'Queue', 'Topic', \n",
    "                  'Notification', 'Event Grid'],\n",
    "    \n",
    "    'monitoring': ['CloudWatch', 'Monitor', 'Log Analytics', 'Stackdriver',\n",
    "                   'Cloud Monitoring', 'X-Ray', 'Application Insights',\n",
    "                   'Logging', 'Metrics', 'Trace', 'Grafana', 'Prometheus'],\n",
    "    \n",
    "    'identity': ['User', 'Client', 'Application', 'Service Principal',\n",
    "                 'OAuth', 'OIDC', 'SAML', 'Directory', 'Active Directory'],\n",
    "    \n",
    "    'ml_ai': ['SageMaker', 'Machine Learning', 'AI Platform', 'Databricks',\n",
    "              'Cognitive Services', 'Vertex AI', 'Rekognition', 'Comprehend',\n",
    "              'Textract', 'Vision', 'Speech', 'Natural Language'],\n",
    "    \n",
    "    'devops': ['CodePipeline', 'CodeBuild', 'CodeDeploy', 'DevOps',\n",
    "               'Cloud Build', 'Artifact Registry', 'Container Registry',\n",
    "               'ECR', 'ACR', 'GCR', 'CI/CD', 'Pipeline', 'Build'],\n",
    "    \n",
    "    'serverless': ['Lambda', 'Functions', 'Azure Functions', 'Cloud Functions',\n",
    "                   'Step Functions', 'Logic Apps', 'Workflows'],\n",
    "    \n",
    "    'analytics': ['Athena', 'BigQuery', 'Synapse', 'Data Factory',\n",
    "                  'Glue', 'Dataflow', 'EMR', 'HDInsight', 'Dataproc',\n",
    "                  'Analytics', 'Data Warehouse', 'ETL']\n",
    "}\n",
    "\n",
    "# Cria mapeamento inverso (case-insensitive)\n",
    "name_to_category = {}\n",
    "for category, keywords in CATEGORY_MAPPING.items():\n",
    "    for keyword in keywords:\n",
    "        name_to_category[keyword.lower()] = category\n",
    "\n",
    "def get_category(class_name):\n",
    "    \"\"\"Mapeia nome de classe para categoria.\"\"\"\n",
    "    class_lower = class_name.lower()\n",
    "    \n",
    "    # Busca exata\n",
    "    if class_lower in name_to_category:\n",
    "        return name_to_category[class_lower]\n",
    "    \n",
    "    # Busca parcial (cont√©m keyword)\n",
    "    for keyword, category in name_to_category.items():\n",
    "        if keyword in class_lower or class_lower in keyword:\n",
    "            return category\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "# Mapeia todas as classes\n",
    "class_to_category = {cls: get_category(cls) for cls in all_classes}\n",
    "\n",
    "# Mostra distribui√ß√£o\n",
    "category_counts = Counter(class_to_category.values())\n",
    "print(\"\\nüìä Distribui√ß√£o por categoria:\")\n",
    "for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"   {cat}: {count} classes\")\n",
    "\n",
    "# Lista classes que foram para \"other\"\n",
    "other_classes = [cls for cls, cat in class_to_category.items() if cat == 'other']\n",
    "if other_classes:\n",
    "    print(f\"\\n‚ö†Ô∏è Classes em 'other' ({len(other_classes)}):\")\n",
    "    for cls in other_classes[:20]:\n",
    "        print(f\"   - {cls}\")\n",
    "\n",
    "# =========================================================\n",
    "# 4. Criar estrutura YOLO e converter anota√ß√µes\n",
    "# =========================================================\n",
    "print(\"\\nüìÅ Criando estrutura YOLO...\")\n",
    "\n",
    "SIMPLIFIED_NAMES = list(CATEGORY_MAPPING.keys()) + ['other']\n",
    "category_to_id = {cat: idx for idx, cat in enumerate(SIMPLIFIED_NAMES)}\n",
    "\n",
    "# Cria estrutura de pastas\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    os.makedirs(f\"dataset/{split}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"dataset/{split}/labels\", exist_ok=True)\n",
    "\n",
    "def convert_voc_to_yolo(xml_file, img_width, img_height):\n",
    "    \"\"\"Converte anota√ß√£o Pascal VOC para formato YOLO.\"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    yolo_lines = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        category = class_to_category.get(class_name, 'other')\n",
    "        class_id = category_to_id[category]\n",
    "        \n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = float(bbox.find('xmin').text)\n",
    "        ymin = float(bbox.find('ymin').text)\n",
    "        xmax = float(bbox.find('xmax').text)\n",
    "        ymax = float(bbox.find('ymax').text)\n",
    "        \n",
    "        # Converte para formato YOLO (centro x, centro y, width, height) normalizado\n",
    "        x_center = (xmin + xmax) / 2 / img_width\n",
    "        y_center = (ymin + ymax) / 2 / img_height\n",
    "        width = (xmax - xmin) / img_width\n",
    "        height = (ymax - ymin) / img_height\n",
    "        \n",
    "        # Garante valores entre 0 e 1\n",
    "        x_center = max(0, min(1, x_center))\n",
    "        y_center = max(0, min(1, y_center))\n",
    "        width = max(0, min(1, width))\n",
    "        height = max(0, min(1, height))\n",
    "        \n",
    "        yolo_lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "    \n",
    "    return yolo_lines\n",
    "\n",
    "# Processa todos os arquivos\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Coleta pares (imagem, xml) - busca recursiva\n",
    "pairs = []\n",
    "for xml_file in xml_files:\n",
    "    img_name = xml_file.stem\n",
    "    xml_dir = xml_file.parent\n",
    "    for ext in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']:\n",
    "        img_path = xml_dir / f\"{img_name}{ext}\"\n",
    "        if img_path.exists():\n",
    "            pairs.append((img_path, xml_file))\n",
    "            break\n",
    "\n",
    "print(f\"   Encontrados {len(pairs)} pares imagem/anota√ß√£o\")\n",
    "\n",
    "if len(pairs) == 0:\n",
    "    print(\"‚ùå Nenhum par imagem/anota√ß√£o encontrado!\")\n",
    "    raise Exception(\"Dataset inv√°lido\")\n",
    "\n",
    "# Shuffle e split (80% train, 10% valid, 10% test)\n",
    "random.seed(42)\n",
    "random.shuffle(pairs)\n",
    "\n",
    "n_train = int(len(pairs) * 0.8)\n",
    "n_valid = int(len(pairs) * 0.1)\n",
    "\n",
    "train_pairs = pairs[:n_train]\n",
    "valid_pairs = pairs[n_train:n_train + n_valid]\n",
    "test_pairs = pairs[n_train + n_valid:]\n",
    "\n",
    "print(f\"   Split: {len(train_pairs)} train, {len(valid_pairs)} valid, {len(test_pairs)} test\")\n",
    "\n",
    "# Converte e copia\n",
    "label_counts = Counter()\n",
    "errors = 0\n",
    "\n",
    "for split, split_pairs in [('train', train_pairs), ('valid', valid_pairs), ('test', test_pairs)]:\n",
    "    for img_path, xml_path in split_pairs:\n",
    "        try:\n",
    "            # L√™ dimens√µes da imagem\n",
    "            with Image.open(img_path) as img:\n",
    "                img_width, img_height = img.size\n",
    "            \n",
    "            # Converte anota√ß√£o\n",
    "            yolo_lines = convert_voc_to_yolo(xml_path, img_width, img_height)\n",
    "            \n",
    "            if yolo_lines:\n",
    "                # Conta labels por categoria\n",
    "                for line in yolo_lines:\n",
    "                    class_id = int(line.split()[0])\n",
    "                    label_counts[SIMPLIFIED_NAMES[class_id]] += 1\n",
    "                \n",
    "                # Copia imagem\n",
    "                dest_img = Path(f\"dataset/{split}/images\") / img_path.name\n",
    "                shutil.copy(img_path, dest_img)\n",
    "                \n",
    "                # Salva label YOLO\n",
    "                label_name = img_path.stem + \".txt\"\n",
    "                dest_label = Path(f\"dataset/{split}/labels\") / label_name\n",
    "                with open(dest_label, \"w\") as f:\n",
    "                    f.write(\"\\n\".join(yolo_lines))\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "\n",
    "if errors > 0:\n",
    "    print(f\"   ‚ö†Ô∏è {errors} arquivos com erro (ignorados)\")\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o de labels por categoria:\")\n",
    "total_labels = sum(label_counts.values())\n",
    "for cat in SIMPLIFIED_NAMES:\n",
    "    count = label_counts.get(cat, 0)\n",
    "    pct = (count / total_labels * 100) if total_labels > 0 else 0\n",
    "    bar = \"#\" * min(50, count // 50)\n",
    "    print(f\"   {cat:12}: {count:5} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(f\"\\n   Total: {total_labels} labels\")\n",
    "\n",
    "# =========================================================\n",
    "# 5. Criar data.yaml\n",
    "# =========================================================\n",
    "data_config = {\n",
    "    'path': '/content/yolo-project/dataset',\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'nc': len(SIMPLIFIED_NAMES),\n",
    "    'names': SIMPLIFIED_NAMES\n",
    "}\n",
    "\n",
    "with open(\"dataset/data.yaml\", \"w\") as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset preparado com {len(SIMPLIFIED_NAMES)} categorias!\")\n",
    "\n",
    "# =========================================================\n",
    "# 6. Verificar Checkpoints Anteriores\n",
    "# =========================================================\n",
    "print(\"\\nüîç Verificando checkpoints anteriores...\")\n",
    "\n",
    "# IMPORTANTE: Limpa checkpoints antigos (modelo antigo tinha classes diferentes)\n",
    "old_checkpoints = glob.glob(f\"{DRIVE_CHECKPOINTS}/*.pt\")\n",
    "if old_checkpoints:\n",
    "    print(\"‚ö†Ô∏è Encontrados checkpoints antigos - limpando...\")\n",
    "    for ckpt in old_checkpoints:\n",
    "        os.remove(ckpt)\n",
    "    print(\"‚úÖ Checkpoints antigos removidos\")\n",
    "\n",
    "last_checkpoint = None\n",
    "print(\"üì≠ Iniciando treinamento do zero com novo dataset\")\n",
    "\n",
    "# =========================================================\n",
    "# 7. Callback para salvar checkpoints\n",
    "# =========================================================\n",
    "SAVE_EVERY_N_EPOCHS = 5\n",
    "\n",
    "def save_checkpoint_to_drive(trainer):\n",
    "    \"\"\"Salva checkpoints no Google Drive.\"\"\"\n",
    "    current_epoch = trainer.epoch + 1\n",
    "    \n",
    "    if current_epoch % SAVE_EVERY_N_EPOCHS == 0:\n",
    "        weights_dir = trainer.save_dir / \"weights\"\n",
    "        \n",
    "        if (weights_dir / \"last.pt\").exists():\n",
    "            epoch_name = f\"epoch_{current_epoch:03d}.pt\"\n",
    "            shutil.copy(weights_dir / \"last.pt\", f\"{DRIVE_CHECKPOINTS}/{epoch_name}\")\n",
    "            shutil.copy(weights_dir / \"last.pt\", f\"{DRIVE_CHECKPOINTS}/last.pt\")\n",
    "            print(f\"\\nüíæ Checkpoint salvo: {epoch_name}\")\n",
    "        \n",
    "        if (weights_dir / \"best.pt\").exists():\n",
    "            shutil.copy(weights_dir / \"best.pt\", f\"{DRIVE_CHECKPOINTS}/best.pt\")\n",
    "\n",
    "# =========================================================\n",
    "# 8. Carregar modelo base\n",
    "# =========================================================\n",
    "print(\"\\nüì¶ Carregando modelo base: yolov8n.pt\")\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "model.add_callback(\"on_train_epoch_end\", save_checkpoint_to_drive)\n",
    "\n",
    "# =========================================================\n",
    "# 9. Treinamento\n",
    "# =========================================================\n",
    "print(\"\\nüöÄ Iniciando treinamento...\")\n",
    "print(f\"   üìä {len(train_pairs)} imagens de treino\")\n",
    "print(f\"   üìä {len(SIMPLIFIED_NAMES)} categorias\")\n",
    "\n",
    "results = model.train(\n",
    "    data=\"dataset/data.yaml\",\n",
    "    \n",
    "    # Configura√ß√£o principal\n",
    "    epochs=100,\n",
    "    patience=20,\n",
    "    batch=16,\n",
    "    imgsz=640,\n",
    "    \n",
    "    # Otimiza√ß√£o\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=3,\n",
    "    cos_lr=True,\n",
    "    \n",
    "    # Augmenta√ß√£o (moderada - dataset j√° tem augmenta√ß√£o)\n",
    "    hsv_h=0.015,\n",
    "    hsv_s=0.4,\n",
    "    hsv_v=0.3,\n",
    "    degrees=10,\n",
    "    translate=0.1,\n",
    "    scale=0.4,\n",
    "    fliplr=0.5,\n",
    "    mosaic=0.8,\n",
    "    mixup=0.1,\n",
    "    \n",
    "    # Loss weights\n",
    "    cls=1.0,\n",
    "    box=7.5,\n",
    "    dfl=1.5,\n",
    "    \n",
    "    # Infraestrutura\n",
    "    cache=True,\n",
    "    workers=4,\n",
    "    device=0,\n",
    "    exist_ok=True,\n",
    "    plots=True,\n",
    "    save_period=5,\n",
    "    \n",
    "    name='train_kaggle',\n",
    "    project='runs/detect',\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 10. Salvar modelo final\n",
    "# =========================================================\n",
    "DEST_WEIGHTS = f\"{DRIVE_PROJECT}/weights_backup\"\n",
    "SOURCE_WEIGHTS = \"runs/detect/train_kaggle/weights\"\n",
    "\n",
    "os.makedirs(DEST_WEIGHTS, exist_ok=True)\n",
    "\n",
    "print(\"\\nüíæ Salvando modelo final...\")\n",
    "\n",
    "if os.path.exists(f\"{SOURCE_WEIGHTS}/best.pt\"):\n",
    "    shutil.copy(f\"{SOURCE_WEIGHTS}/best.pt\", f\"{DEST_WEIGHTS}/best_kaggle.pt\")\n",
    "    shutil.copy(f\"{SOURCE_WEIGHTS}/best.pt\", f\"{DRIVE_CHECKPOINTS}/best_final.pt\")\n",
    "    \n",
    "    with open(f\"{DEST_WEIGHTS}/class_mapping_kaggle.yaml\", \"w\") as f:\n",
    "        yaml.dump({\n",
    "            'simplified_names': SIMPLIFIED_NAMES,\n",
    "            'category_mapping': CATEGORY_MAPPING,\n",
    "            'original_classes': list(all_classes)\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo salvo: {DEST_WEIGHTS}/best_kaggle.pt\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è best.pt n√£o encontrado\")\n",
    "\n",
    "# Limpa checkpoints antigos (mant√©m apenas √∫ltimos 3)\n",
    "checkpoint_files = sorted(glob.glob(f\"{DRIVE_CHECKPOINTS}/epoch*.pt\"))\n",
    "if len(checkpoint_files) > 3:\n",
    "    for old_ckpt in checkpoint_files[:-3]:\n",
    "        os.remove(old_ckpt)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nüìÅ Modelo final: {DEST_WEIGHTS}/best_kaggle.pt\")\n",
    "print(\"   Baixe esse arquivo e coloque em models/best.pt no seu PC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f3cba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Validando modelo treinado...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     11\u001b[39m best_model_path = \u001b[33m\"\u001b[39m\u001b[33mruns/detect/train_mvp_optimized/weights/best.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mos\u001b[49m.path.exists(best_model_path):\n\u001b[32m     14\u001b[39m     val_model = YOLO(best_model_path)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Executa valida√ß√£o no conjunto de teste\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 11. VALIDA√á√ÉO DO MODELO TREINADO\n",
    "# =========================================================\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Configura√ß√£o\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/colab\"\n",
    "PROJECT_NAME = \"cloud-arch-security-mvp\"\n",
    "DRIVE_PROJECT = f\"{DRIVE_ROOT}/{PROJECT_NAME}\"\n",
    "DRIVE_CHECKPOINTS = f\"{DRIVE_PROJECT}/checkpoints\"\n",
    "\n",
    "# Monta Drive se necess√°rio\n",
    "from google.colab import drive\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# Procura o modelo\n",
    "print(\"\\nüîç Procurando modelo treinado...\")\n",
    "\n",
    "model_paths = [\n",
    "    \"runs/detect/train_kaggle/weights/best.pt\",\n",
    "    f\"{DRIVE_CHECKPOINTS}/best.pt\",\n",
    "    f\"{DRIVE_CHECKPOINTS}/best_final.pt\",\n",
    "    f\"{DRIVE_PROJECT}/weights_backup/best_kaggle.pt\",\n",
    "]\n",
    "\n",
    "best_model_path = None\n",
    "for path in model_paths:\n",
    "    if os.path.exists(path):\n",
    "        best_model_path = path\n",
    "        print(f\"‚úÖ Modelo encontrado: {path}\")\n",
    "        break\n",
    "\n",
    "if not best_model_path:\n",
    "    print(\"‚ùå Nenhum modelo encontrado!\")\n",
    "else:\n",
    "    val_model = YOLO(best_model_path)\n",
    "    print(f\"üìä Modelo tem {len(val_model.names)} classes:\")\n",
    "    for idx, name in val_model.names.items():\n",
    "        print(f\"   {idx}: {name}\")\n",
    "    \n",
    "    # Valida√ß√£o\n",
    "    print(\"\\nüß™ Validando modelo...\")\n",
    "    \n",
    "    val_results = val_model.val(\n",
    "        data=\"dataset/data.yaml\",\n",
    "        split=\"test\",\n",
    "        plots=True,\n",
    "        save_json=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä M√âTRICAS DE VALIDA√á√ÉO:\")\n",
    "    print(f\"   mAP50: {val_results.box.map50:.4f}\")\n",
    "    print(f\"   mAP50-95: {val_results.box.map:.4f}\")\n",
    "    print(f\"   Precis√£o: {val_results.box.mp:.4f}\")\n",
    "    print(f\"   Recall: {val_results.box.mr:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìà mAP50 por categoria:\")\n",
    "    for i, name in enumerate(val_model.names.values()):\n",
    "        if i < len(val_results.box.ap50):\n",
    "            ap = val_results.box.ap50[i]\n",
    "            print(f\"   {name}: {ap:.4f}\")\n",
    "\n",
    "    # Teste visual\n",
    "    print(\"\\nüñºÔ∏è Testando em imagem de exemplo...\")\n",
    "    \n",
    "    test_images = list(Path(\"dataset/test/images\").glob(\"*.png\")) + \\\n",
    "                  list(Path(\"dataset/test/images\").glob(\"*.jpg\"))\n",
    "    \n",
    "    if test_images:\n",
    "        # Testa 3 imagens aleat√≥rias\n",
    "        for test_img in random.sample(test_images, min(3, len(test_images))):\n",
    "            print(f\"\\n   üì∑ {test_img.name}\")\n",
    "            \n",
    "            results = val_model(str(test_img), conf=0.25, verbose=False)\n",
    "            \n",
    "            detected = set()\n",
    "            for r in results:\n",
    "                for box in r.boxes:\n",
    "                    cls_name = val_model.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "                    detected.add(f\"{cls_name} ({conf:.2f})\")\n",
    "            \n",
    "            if detected:\n",
    "                print(f\"      Detectado: {', '.join(detected)}\")\n",
    "            else:\n",
    "                print(\"      ‚ö†Ô∏è Nenhuma detec√ß√£o\")\n",
    "            \n",
    "            # Exibe\n",
    "            result_img = results[0].plot()\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(result_img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Detec√ß√µes em {test_img.name}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Nenhuma imagem de teste encontrada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
