{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# INSTALA√á√ÉO\n",
    "# =========================================================\n",
    "!pip install ultralytics pillow\n",
    "\n",
    "print(\"‚úÖ Depend√™ncias instaladas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbe76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import yaml\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import zipfile\n",
    "\n",
    "# =========================================================\n",
    "# 0. Mount Google Drive\n",
    "# =========================================================\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/colab\"\n",
    "PROJECT_NAME = \"cloud-arch-security-mvp\"\n",
    "\n",
    "DRIVE_PROJECT = f\"{DRIVE_ROOT}/{PROJECT_NAME}\"\n",
    "DRIVE_CHECKPOINTS = f\"{DRIVE_PROJECT}/checkpoints\"\n",
    "DRIVE_DATASET_ZIP = f\"{DRIVE_PROJECT}/kaggle_dataset_cache/dataset_ready.zip\"  # ZIP preparado com prepare_dataset.py\n",
    "\n",
    "CONTENT_PROJECT = \"/content/yolo-project\"\n",
    "\n",
    "os.makedirs(DRIVE_CHECKPOINTS, exist_ok=True)\n",
    "\n",
    "# =========================================================\n",
    "# 1. Carregar Dataset do Drive (via ZIP - muito mais r√°pido!)\n",
    "# =========================================================\n",
    "print(\"üì• Carregando dataset do Google Drive...\")\n",
    "\n",
    "# Limpa ambiente anterior\n",
    "if Path(CONTENT_PROJECT).exists():\n",
    "    shutil.rmtree(CONTENT_PROJECT)\n",
    "os.makedirs(CONTENT_PROJECT, exist_ok=True)\n",
    "\n",
    "RAW_DATA_PATH = f\"{CONTENT_PROJECT}/raw_data\"\n",
    "\n",
    "# Verifica se o ZIP existe\n",
    "if not os.path.exists(DRIVE_DATASET_ZIP):\n",
    "    print(\"‚ùå Dataset ZIP n√£o encontrado!\")\n",
    "    print(f\"   Esperado em: {DRIVE_DATASET_ZIP}\")\n",
    "    print(\"\\nüìã Instru√ß√µes:\")\n",
    "    print(\"   1. No seu PC, execute: python prepare_dataset.py\")\n",
    "    print(\"   2. Fa√ßa upload de 'dataset_ready.zip' para o Google Drive em:\")\n",
    "    print(f\"      {DRIVE_DATASET_ZIP}\")\n",
    "    raise Exception(\"Dataset ZIP n√£o encontrado no Drive\")\n",
    "\n",
    "# Copia ZIP para o Colab (1 arquivo = r√°pido!)\n",
    "print(\"   üì¶ Copiando ZIP para ambiente local...\")\n",
    "zip_size_mb = os.path.getsize(DRIVE_DATASET_ZIP) / (1024 * 1024)\n",
    "print(f\"      Tamanho: {zip_size_mb:.1f} MB\")\n",
    "\n",
    "local_zip = \"/content/dataset.zip\"\n",
    "shutil.copy(DRIVE_DATASET_ZIP, local_zip)\n",
    "print(\"   ‚úÖ ZIP copiado!\")\n",
    "\n",
    "# Descompacta localmente (SSD do Colab = muito r√°pido!)\n",
    "print(\"   üìÇ Descompactando...\")\n",
    "os.makedirs(RAW_DATA_PATH, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(local_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(RAW_DATA_PATH)\n",
    "\n",
    "# Remove ZIP para liberar espa√ßo\n",
    "os.remove(local_zip)\n",
    "\n",
    "# Verifica se extraiu corretamente (pode ter subpasta)\n",
    "extracted_items = os.listdir(RAW_DATA_PATH)\n",
    "if len(extracted_items) == 1 and os.path.isdir(f\"{RAW_DATA_PATH}/{extracted_items[0]}\"):\n",
    "    # Se extraiu para uma subpasta, move os arquivos\n",
    "    subfolder = f\"{RAW_DATA_PATH}/{extracted_items[0]}\"\n",
    "    for item in os.listdir(subfolder):\n",
    "        shutil.move(f\"{subfolder}/{item}\", RAW_DATA_PATH)\n",
    "    os.rmdir(subfolder)\n",
    "\n",
    "file_count = len(os.listdir(RAW_DATA_PATH))\n",
    "print(f\"‚úÖ Dataset carregado! ({file_count} arquivos)\")\n",
    "\n",
    "os.chdir(CONTENT_PROJECT)\n",
    "\n",
    "# =========================================================\n",
    "# 2. Converter Pascal VOC (XML) para YOLO Format\n",
    "# =========================================================\n",
    "print(\"\\nüîÑ Convertendo Pascal VOC para YOLO format...\")\n",
    "\n",
    "RAW_DATA = Path(\"raw_data\")\n",
    "\n",
    "# Coleta todas as classes do dataset\n",
    "all_classes = set()\n",
    "xml_files = list(RAW_DATA.glob(\"**/*.xml\"))  # Busca recursiva\n",
    "print(f\"   Encontrados {len(xml_files)} arquivos XML\")\n",
    "\n",
    "if len(xml_files) == 0:\n",
    "    print(\"‚ùå Nenhum arquivo XML encontrado!\")\n",
    "    print(f\"   Verificando conte√∫do de {RAW_DATA}:\")\n",
    "    for i, item in enumerate(RAW_DATA.iterdir()):\n",
    "        print(f\"      - {item.name}\")\n",
    "        if i > 20:\n",
    "            print(\"      ... (mais arquivos)\")\n",
    "            break\n",
    "    raise Exception(\"Dataset inv√°lido - sem arquivos XML\")\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    try:\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for obj in root.findall('object'):\n",
    "            class_name = obj.find('name').text\n",
    "            all_classes.add(class_name)\n",
    "    except Exception as e:\n",
    "        pass  # Ignora erros silenciosamente\n",
    "\n",
    "all_classes = sorted(list(all_classes))\n",
    "print(f\"   Total de classes encontradas: {len(all_classes)}\")\n",
    "\n",
    "# =========================================================\n",
    "# 3. Mapear para 14 Categorias STRIDE\n",
    "# =========================================================\n",
    "print(\"\\nüìä Mapeando para categorias STRIDE...\")\n",
    "\n",
    "# Mapeamento COMPLETO incluindo nomes com prefixos AWS/Azure/GCP\n",
    "CATEGORY_MAPPING = {\n",
    "    'compute': [\n",
    "        'EC2', 'Lambda', 'EKS', 'Fargate', 'Container', 'ECS',\n",
    "        'App Service', 'Virtual Machine', 'VM', 'Compute Engine',\n",
    "        'Cloud Run', 'App Engine', 'GKE', 'AKS', 'Kubernetes',\n",
    "        'Elastic Beanstalk', 'Batch', 'Lightsail', 'EMR',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_ec2', 'aws_amazon_ec2_instance', 'aws_ec2',\n",
    "        'aws_auto_scaling', 'aws_autoscaling', 'aws_elastic_beanstalk',\n",
    "        'aws_amazon_elastic_container_service', 'aws_amazon_eks',\n",
    "        'azure_virtual_machines', 'azure_app_services', 'azure_vm',\n",
    "        'azure_container_instances', 'azure_kubernetes_service',\n",
    "        'gcp_compute_engine', 'gcp_gke', 'gcp_cloud_run',\n",
    "    ],\n",
    "    \n",
    "    'database': [\n",
    "        'RDS', 'DynamoDB', 'Aurora', 'DocumentDB', 'ElastiCache',\n",
    "        'Cosmos DB', 'SQL Database', 'Cloud SQL', 'Firestore',\n",
    "        'BigQuery', 'Redshift', 'Neptune', 'Cloud Spanner',\n",
    "        'Managed Database', 'Database', 'DB', 'Redis', 'Memcached',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_rds', 'aws_amazon_dynamodb', 'aws_amazon_aurora',\n",
    "        'aws_amazon_elasticache', 'aws_amazon_redshift',\n",
    "        'azure_sql_database', 'azure_cosmos_db', 'azure_sql_databases',\n",
    "        'gcp_cloud_sql', 'gcp_cloud_spanner', 'gcp_firestore',\n",
    "    ],\n",
    "    \n",
    "    'storage': [\n",
    "        'S3', 'EBS', 'EFS', 'Glacier', 'Storage', 'Blob Storage',\n",
    "        'Cloud Storage', 'File Storage', 'Azure Storage', 'GCS',\n",
    "        'Backup', 'Archive', 'Data Lake',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_s3', 'aws_amazon_simple_storage_service',\n",
    "        'aws_amazon_elastic_block_store', 'aws_elastic_block_store',\n",
    "        'aws_elastic_block_store_volume', 'aws_elactic_file_system',\n",
    "        'aws_amazon_glacier', 'aws_amazon_efs',\n",
    "        'azure_blob_storage', 'azure_storage_accounts', 'azure_files',\n",
    "        'gcp_cloud_storage', 'gcp_persistent_disk',\n",
    "    ],\n",
    "    \n",
    "    'network': [\n",
    "        'VPC', 'Virtual Network', 'VNet', 'Subnet', 'Gateway',\n",
    "        'Load Balancer', 'ALB', 'NLB', 'ELB', 'CloudFront',\n",
    "        'CDN', 'Route 53', 'DNS', 'VPN', 'Direct Connect',\n",
    "        'ExpressRoute', 'Cloud Interconnect', 'NAT', 'Firewall',\n",
    "        'Network', 'Internet', 'Internet Gateway', 'Transit Gateway',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_vpc', 'aws_amazon_virtual_private_cloud',\n",
    "        'aws_virtual_private_cloud', 'aws_amazon_route_53',\n",
    "        'aws_route_53_hosted_zone', 'aws_amazon_cloudfront',\n",
    "        'aws_elastic_load_balancing', 'aws_elb', 'aws_alb', 'aws_nlb',\n",
    "        'aws_internet_gateway', 'aws_nat_gateway', 'aws_transit_gateway',\n",
    "        'aws_region', 'aws_availability_zone',\n",
    "        'azure_virtual_network', 'azure_load_balancer', 'azure_cdn',\n",
    "        'azure_application_gateway', 'azure_traffic_manager',\n",
    "        'azure_expressroute', 'azure_vpn_gateway', 'azure_firewall',\n",
    "        'gcp_vpc', 'gcp_cloud_load_balancing', 'gcp_cloud_cdn',\n",
    "    ],\n",
    "    \n",
    "    'security': [\n",
    "        'IAM', 'Identity', 'Cognito', 'WAF', 'Shield', 'GuardDuty',\n",
    "        'Security Hub', 'Key Vault', 'KMS', 'Secrets Manager',\n",
    "        'Certificate', 'Azure AD', 'Cloud Identity', 'SSO',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_iam', 'aws_iam', 'aws_amazon_cognito',\n",
    "        'aws_key_management_service', 'aws_amazon_kms',\n",
    "        'aws_secrets_manager', 'aws_waf', 'aws_shield',\n",
    "        'aws_amazon_guardduty', 'aws_security_hub',\n",
    "        'azure_key_vault', 'azure_active_directory', 'azure_ad',\n",
    "        'azure_security_center', 'azure_sentinel',\n",
    "        'gcp_cloud_iam', 'gcp_secret_manager',\n",
    "    ],\n",
    "    \n",
    "    'api_gateway': [\n",
    "        'API Gateway', 'API Management', 'Apigee', 'AppSync',\n",
    "        'API', 'Gateway', 'Endpoints',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_api_gateway', 'aws_api_gateway',\n",
    "        'azure_api_management', 'azure_api_apps',\n",
    "        'gcp_apigee', 'gcp_cloud_endpoints',\n",
    "    ],\n",
    "    \n",
    "    'messaging': [\n",
    "        'SQS', 'SNS', 'SES', 'EventBridge', 'Service Bus', 'Pub/Sub',\n",
    "        'Kinesis', 'Event Hub', 'MQ', 'Queue', 'Topic',\n",
    "        'Notification', 'Event Grid', 'Email',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_sqs', 'aws_amazon_sns', 'aws_amazon_ses',\n",
    "        'aws_amazon_eventbridge', 'aws_amazon_kinesis',\n",
    "        'aws_amazon_mq', 'aws_simple_queue_service',\n",
    "        'aws_simple_notification_service', 'aws_simple_email_service',\n",
    "        'azure_service_bus', 'azure_event_hubs', 'azure_event_grid',\n",
    "        'azure_notification_hubs', 'azure_queue_storage',\n",
    "        'gcp_pub_sub', 'gcp_cloud_tasks',\n",
    "    ],\n",
    "    \n",
    "    'monitoring': [\n",
    "        'CloudWatch', 'CloudTrail', 'Monitor', 'Log Analytics', 'Stackdriver',\n",
    "        'Cloud Monitoring', 'X-Ray', 'Application Insights',\n",
    "        'Logging', 'Metrics', 'Trace', 'Grafana', 'Prometheus', 'Audit',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_cloudwatch', 'aws_cloudwatch',\n",
    "        'aws_cloud_trail', 'aws_cloudtrail', 'aws_amazon_cloudtrail',\n",
    "        'aws_x-ray', 'aws_xray',\n",
    "        'azure_monitor', 'azure_application_insights', 'azure_log_analytics',\n",
    "        'gcp_cloud_monitoring', 'gcp_cloud_logging', 'gcp_cloud_trace',\n",
    "    ],\n",
    "    \n",
    "    'identity': [\n",
    "        'User', 'Client', 'Application', 'Service Principal',\n",
    "        'OAuth', 'OIDC', 'SAML', 'Directory', 'Active Directory',\n",
    "    ],\n",
    "    \n",
    "    'ml_ai': [\n",
    "        'SageMaker', 'Machine Learning', 'AI Platform', 'Databricks',\n",
    "        'Cognitive Services', 'Vertex AI', 'Rekognition', 'Comprehend',\n",
    "        'Textract', 'Vision', 'Speech', 'Natural Language',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_sagemaker', 'aws_sagemaker',\n",
    "        'azure_machine_learning', 'azure_cognitive_services',\n",
    "        'gcp_vertex_ai', 'gcp_automl',\n",
    "    ],\n",
    "    \n",
    "    'devops': [\n",
    "        'CodePipeline', 'CodeBuild', 'CodeDeploy', 'DevOps',\n",
    "        'Cloud Build', 'Artifact Registry', 'Container Registry',\n",
    "        'ECR', 'ACR', 'GCR', 'CI/CD', 'Pipeline', 'Build',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_codepipeline', 'aws_codebuild', 'aws_codedeploy',\n",
    "        'aws_codecommit', 'aws_amazon_ecr',\n",
    "        'aws_cloudformation', 'aws_cloudformation_template',\n",
    "        'azure_devops', 'azure_container_registry', 'azure_pipelines',\n",
    "        'gcp_cloud_build', 'gcp_artifact_registry',\n",
    "    ],\n",
    "    \n",
    "    'serverless': [\n",
    "        'Lambda', 'Functions', 'Azure Functions', 'Cloud Functions',\n",
    "        'Step Functions', 'Logic Apps', 'Workflows',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_lambda', 'aws_lambda', 'aws_step_functions',\n",
    "        'azure_functions', 'azure_function_apps', 'azure_logic_apps',\n",
    "        'gcp_cloud_functions',\n",
    "    ],\n",
    "    \n",
    "    'analytics': [\n",
    "        'Athena', 'BigQuery', 'Synapse', 'Data Factory',\n",
    "        'Glue', 'Dataflow', 'EMR', 'HDInsight', 'Dataproc',\n",
    "        'Analytics', 'Data Warehouse', 'ETL',\n",
    "        # Nomes completos do dataset\n",
    "        'aws_amazon_athena', 'aws_amazon_emr', 'aws_glue',\n",
    "        'aws_amazon_kinesis_data_firehose', 'aws_amazon_quicksight',\n",
    "        'azure_synapse_analytics', 'azure_data_factories',\n",
    "        'azure_data_lake', 'azure_hdinsight', 'azure_stream_analytics',\n",
    "        'gcp_bigquery', 'gcp_dataflow', 'gcp_dataproc',\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Cria mapeamento inverso (case-insensitive)\n",
    "name_to_category = {}\n",
    "for category, keywords in CATEGORY_MAPPING.items():\n",
    "    for keyword in keywords:\n",
    "        name_to_category[keyword.lower()] = category\n",
    "\n",
    "def get_category(class_name):\n",
    "    \"\"\"Mapeia nome de classe para categoria.\"\"\"\n",
    "    class_lower = class_name.lower()\n",
    "    \n",
    "    # Busca exata primeiro\n",
    "    if class_lower in name_to_category:\n",
    "        return name_to_category[class_lower]\n",
    "    \n",
    "    # Busca parcial - verifica se alguma keyword est√° contida no nome\n",
    "    for keyword, category in name_to_category.items():\n",
    "        # Ignora keywords muito curtas para evitar falsos positivos\n",
    "        if len(keyword) < 3:\n",
    "            continue\n",
    "        if keyword in class_lower:\n",
    "            return category\n",
    "    \n",
    "    # Busca por padr√µes comuns\n",
    "    patterns = {\n",
    "        'ec2': 'compute',\n",
    "        'lambda': 'serverless',\n",
    "        's3': 'storage',\n",
    "        'rds': 'database',\n",
    "        'vpc': 'network',\n",
    "        'iam': 'security',\n",
    "        'sqs': 'messaging',\n",
    "        'sns': 'messaging',\n",
    "        'cloudwatch': 'monitoring',\n",
    "        'cloudtrail': 'monitoring',\n",
    "        'api_gateway': 'api_gateway',\n",
    "        'load_balanc': 'network',\n",
    "        'elastic_block': 'storage',\n",
    "        'route_53': 'network',\n",
    "        'route53': 'network',\n",
    "        'cloudfront': 'network',\n",
    "        'cognito': 'security',\n",
    "        'kms': 'security',\n",
    "        'key_management': 'security',\n",
    "        'secrets': 'security',\n",
    "        'dynamodb': 'database',\n",
    "        'elasticache': 'database',\n",
    "        'redshift': 'database',\n",
    "        'aurora': 'database',\n",
    "        'glacier': 'storage',\n",
    "        'efs': 'storage',\n",
    "        'ebs': 'storage',\n",
    "        'kinesis': 'messaging',\n",
    "        'eventbridge': 'messaging',\n",
    "        'event_hub': 'messaging',\n",
    "        'service_bus': 'messaging',\n",
    "        'sagemaker': 'ml_ai',\n",
    "        'machine_learning': 'ml_ai',\n",
    "        'codepipeline': 'devops',\n",
    "        'codebuild': 'devops',\n",
    "        'cloudformation': 'devops',\n",
    "        'step_function': 'serverless',\n",
    "        'logic_app': 'serverless',\n",
    "        'function_app': 'serverless',\n",
    "        'data_factor': 'analytics',\n",
    "        'synapse': 'analytics',\n",
    "        'athena': 'analytics',\n",
    "        'glue': 'analytics',\n",
    "        'bigquery': 'analytics',\n",
    "        'virtual_machine': 'compute',\n",
    "        'app_service': 'compute',\n",
    "        'container': 'compute',\n",
    "        'kubernetes': 'compute',\n",
    "        'eks': 'compute',\n",
    "        'aks': 'compute',\n",
    "        'gke': 'compute',\n",
    "    }\n",
    "    \n",
    "    for pattern, category in patterns.items():\n",
    "        if pattern in class_lower:\n",
    "            return category\n",
    "    \n",
    "    return 'other'\n",
    "\n",
    "# Mapeia todas as classes\n",
    "class_to_category = {cls: get_category(cls) for cls in all_classes}\n",
    "\n",
    "# Mostra distribui√ß√£o\n",
    "category_counts = Counter(class_to_category.values())\n",
    "print(\"\\nüìä Distribui√ß√£o por categoria:\")\n",
    "for cat, count in sorted(category_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"   {cat}: {count} classes\")\n",
    "\n",
    "# Lista classes que foram para \"other\"\n",
    "other_classes = [cls for cls, cat in class_to_category.items() if cat == 'other']\n",
    "if other_classes:\n",
    "    print(f\"\\n‚ö†Ô∏è Classes em 'other' ({len(other_classes)}):\")\n",
    "    for cls in other_classes[:20]:\n",
    "        print(f\"   - {cls}\")\n",
    "    if len(other_classes) > 20:\n",
    "        print(f\"   ... e mais {len(other_classes) - 20}\")\n",
    "\n",
    "# =========================================================\n",
    "# 4. Criar estrutura YOLO e converter anota√ß√µes\n",
    "# =========================================================\n",
    "print(\"\\nüìÅ Criando estrutura YOLO...\")\n",
    "\n",
    "SIMPLIFIED_NAMES = list(CATEGORY_MAPPING.keys()) + ['other']\n",
    "category_to_id = {cat: idx for idx, cat in enumerate(SIMPLIFIED_NAMES)}\n",
    "\n",
    "# Cria estrutura de pastas\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    os.makedirs(f\"dataset/{split}/images\", exist_ok=True)\n",
    "    os.makedirs(f\"dataset/{split}/labels\", exist_ok=True)\n",
    "\n",
    "def convert_voc_to_yolo(xml_file, img_width, img_height):\n",
    "    \"\"\"Converte anota√ß√£o Pascal VOC para formato YOLO.\"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    yolo_lines = []\n",
    "    for obj in root.findall('object'):\n",
    "        class_name = obj.find('name').text\n",
    "        category = class_to_category.get(class_name, 'other')\n",
    "        class_id = category_to_id[category]\n",
    "        \n",
    "        bbox = obj.find('bndbox')\n",
    "        xmin = float(bbox.find('xmin').text)\n",
    "        ymin = float(bbox.find('ymin').text)\n",
    "        xmax = float(bbox.find('xmax').text)\n",
    "        ymax = float(bbox.find('ymax').text)\n",
    "        \n",
    "        # Converte para formato YOLO (centro x, centro y, width, height) normalizado\n",
    "        x_center = (xmin + xmax) / 2 / img_width\n",
    "        y_center = (ymin + ymax) / 2 / img_height\n",
    "        width = (xmax - xmin) / img_width\n",
    "        height = (ymax - ymin) / img_height\n",
    "        \n",
    "        # Garante valores entre 0 e 1\n",
    "        x_center = max(0, min(1, x_center))\n",
    "        y_center = max(0, min(1, y_center))\n",
    "        width = max(0, min(1, width))\n",
    "        height = max(0, min(1, height))\n",
    "        \n",
    "        yolo_lines.append(f\"{class_id} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "    \n",
    "    return yolo_lines\n",
    "\n",
    "# Processa todos os arquivos\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Coleta pares (imagem, xml) - busca recursiva\n",
    "pairs = []\n",
    "for xml_file in xml_files:\n",
    "    img_name = xml_file.stem\n",
    "    xml_dir = xml_file.parent\n",
    "    for ext in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']:\n",
    "        img_path = xml_dir / f\"{img_name}{ext}\"\n",
    "        if img_path.exists():\n",
    "            pairs.append((img_path, xml_file))\n",
    "            break\n",
    "\n",
    "print(f\"   Encontrados {len(pairs)} pares imagem/anota√ß√£o\")\n",
    "\n",
    "if len(pairs) == 0:\n",
    "    print(\"‚ùå Nenhum par imagem/anota√ß√£o encontrado!\")\n",
    "    raise Exception(\"Dataset inv√°lido\")\n",
    "\n",
    "# Shuffle e split (80% train, 10% valid, 10% test)\n",
    "random.seed(42)\n",
    "random.shuffle(pairs)\n",
    "\n",
    "n_train = int(len(pairs) * 0.8)\n",
    "n_valid = int(len(pairs) * 0.1)\n",
    "\n",
    "train_pairs = pairs[:n_train]\n",
    "valid_pairs = pairs[n_train:n_train + n_valid]\n",
    "test_pairs = pairs[n_train + n_valid:]\n",
    "\n",
    "print(f\"   Split: {len(train_pairs)} train, {len(valid_pairs)} valid, {len(test_pairs)} test\")\n",
    "\n",
    "# Converte e copia\n",
    "label_counts = Counter()\n",
    "errors = 0\n",
    "\n",
    "for split, split_pairs in [('train', train_pairs), ('valid', valid_pairs), ('test', test_pairs)]:\n",
    "    for img_path, xml_path in split_pairs:\n",
    "        try:\n",
    "            # L√™ dimens√µes da imagem\n",
    "            with Image.open(img_path) as img:\n",
    "                img_width, img_height = img.size\n",
    "            \n",
    "            # Converte anota√ß√£o\n",
    "            yolo_lines = convert_voc_to_yolo(xml_path, img_width, img_height)\n",
    "            \n",
    "            if yolo_lines:\n",
    "                # Conta labels por categoria\n",
    "                for line in yolo_lines:\n",
    "                    class_id = int(line.split()[0])\n",
    "                    label_counts[SIMPLIFIED_NAMES[class_id]] += 1\n",
    "                \n",
    "                # Copia imagem\n",
    "                dest_img = Path(f\"dataset/{split}/images\") / img_path.name\n",
    "                shutil.copy(img_path, dest_img)\n",
    "                \n",
    "                # Salva label YOLO\n",
    "                label_name = img_path.stem + \".txt\"\n",
    "                dest_label = Path(f\"dataset/{split}/labels\") / label_name\n",
    "                with open(dest_label, \"w\") as f:\n",
    "                    f.write(\"\\n\".join(yolo_lines))\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "\n",
    "if errors > 0:\n",
    "    print(f\"   ‚ö†Ô∏è {errors} arquivos com erro (ignorados)\")\n",
    "\n",
    "print(\"\\nüìä Distribui√ß√£o de labels por categoria:\")\n",
    "total_labels = sum(label_counts.values())\n",
    "for cat in SIMPLIFIED_NAMES:\n",
    "    count = label_counts.get(cat, 0)\n",
    "    pct = (count / total_labels * 100) if total_labels > 0 else 0\n",
    "    bar = \"#\" * min(50, count // 50)\n",
    "    print(f\"   {cat:12}: {count:5} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "print(f\"\\n   Total: {total_labels} labels\")\n",
    "\n",
    "# =========================================================\n",
    "# 5. Criar data.yaml\n",
    "# =========================================================\n",
    "data_config = {\n",
    "    'path': '/content/yolo-project/dataset',\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'nc': len(SIMPLIFIED_NAMES),\n",
    "    'names': SIMPLIFIED_NAMES\n",
    "}\n",
    "\n",
    "with open(\"dataset/data.yaml\", \"w\") as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset preparado com {len(SIMPLIFIED_NAMES)} categorias!\")\n",
    "\n",
    "# =========================================================\n",
    "# 6. Verificar Checkpoints Anteriores (RETOMAR TREINAMENTO)\n",
    "# =========================================================\n",
    "print(\"\\nüîç Verificando checkpoints anteriores...\")\n",
    "\n",
    "# Procura checkpoint mais recente para retomar\n",
    "checkpoint_files = sorted(glob.glob(f\"{DRIVE_CHECKPOINTS}/epoch*.pt\"))\n",
    "last_checkpoint = None\n",
    "resume_training = False\n",
    "\n",
    "if os.path.exists(f\"{DRIVE_CHECKPOINTS}/last.pt\"):\n",
    "    last_checkpoint = f\"{DRIVE_CHECKPOINTS}/last.pt\"\n",
    "    resume_training = True\n",
    "    print(f\"‚úÖ Checkpoint encontrado: last.pt\")\n",
    "    print(\"   üîÑ Treinamento ser√° RETOMADO do √∫ltimo checkpoint!\")\n",
    "elif checkpoint_files:\n",
    "    last_checkpoint = checkpoint_files[-1]\n",
    "    resume_training = True\n",
    "    print(f\"‚úÖ Checkpoint encontrado: {os.path.basename(last_checkpoint)}\")\n",
    "    print(\"   üîÑ Treinamento ser√° RETOMADO deste checkpoint!\")\n",
    "else:\n",
    "    print(\"üì≠ Nenhum checkpoint encontrado - iniciando do zero\")\n",
    "\n",
    "# =========================================================\n",
    "# 7. Callback para salvar checkpoints\n",
    "# =========================================================\n",
    "SAVE_EVERY_N_EPOCHS = 5\n",
    "\n",
    "def save_checkpoint_to_drive(trainer):\n",
    "    \"\"\"Salva checkpoints no Google Drive.\"\"\"\n",
    "    current_epoch = trainer.epoch + 1\n",
    "    \n",
    "    if current_epoch % SAVE_EVERY_N_EPOCHS == 0:\n",
    "        weights_dir = trainer.save_dir / \"weights\"\n",
    "        \n",
    "        if (weights_dir / \"last.pt\").exists():\n",
    "            epoch_name = f\"epoch_{current_epoch:03d}.pt\"\n",
    "            shutil.copy(weights_dir / \"last.pt\", f\"{DRIVE_CHECKPOINTS}/{epoch_name}\")\n",
    "            shutil.copy(weights_dir / \"last.pt\", f\"{DRIVE_CHECKPOINTS}/last.pt\")\n",
    "            print(f\"\\nüíæ Checkpoint salvo: {epoch_name}\")\n",
    "        \n",
    "        if (weights_dir / \"best.pt\").exists():\n",
    "            shutil.copy(weights_dir / \"best.pt\", f\"{DRIVE_CHECKPOINTS}/best.pt\")\n",
    "\n",
    "# =========================================================\n",
    "# 8. Carregar modelo (do checkpoint ou base)\n",
    "# =========================================================\n",
    "if resume_training and last_checkpoint:\n",
    "    print(f\"\\nüì¶ Carregando checkpoint: {last_checkpoint}\")\n",
    "    model = YOLO(last_checkpoint)\n",
    "else:\n",
    "    print(\"\\nüì¶ Carregando modelo base: yolov8n.pt\")\n",
    "    model = YOLO(\"yolov8n.pt\")\n",
    "\n",
    "model.add_callback(\"on_train_epoch_end\", save_checkpoint_to_drive)\n",
    "\n",
    "# =========================================================\n",
    "# 9. Treinamento\n",
    "# =========================================================\n",
    "print(\"\\nüöÄ Iniciando treinamento...\")\n",
    "print(f\"   üìä {len(train_pairs)} imagens de treino\")\n",
    "print(f\"   üìä {len(SIMPLIFIED_NAMES)} categorias\")\n",
    "if resume_training:\n",
    "    print(\"   üîÑ RETOMANDO do checkpoint anterior!\")\n",
    "\n",
    "results = model.train(\n",
    "    data=\"dataset/data.yaml\",\n",
    "    \n",
    "    # Configura√ß√£o principal\n",
    "    epochs=150,\n",
    "    patience=30,\n",
    "    batch=16,\n",
    "    imgsz=640,\n",
    "    \n",
    "    # Retomar treinamento\n",
    "    resume=resume_training,\n",
    "    \n",
    "    # Otimiza√ß√£o\n",
    "    optimizer='AdamW',\n",
    "    lr0=0.001,\n",
    "    lrf=0.01,\n",
    "    weight_decay=0.0005,\n",
    "    warmup_epochs=3,\n",
    "    cos_lr=True,\n",
    "    \n",
    "    # Augmenta√ß√£o (moderada - dataset j√° tem augmenta√ß√£o)\n",
    "    hsv_h=0.015,\n",
    "    hsv_s=0.4,\n",
    "    hsv_v=0.3,\n",
    "    degrees=10,\n",
    "    translate=0.1,\n",
    "    scale=0.4,\n",
    "    fliplr=0.5,\n",
    "    mosaic=0.8,\n",
    "    mixup=0.1,\n",
    "    \n",
    "    # Loss weights\n",
    "    cls=1.0,\n",
    "    box=7.5,\n",
    "    dfl=1.5,\n",
    "    \n",
    "    # Infraestrutura\n",
    "    cache=True,\n",
    "    workers=4,\n",
    "    device=0,\n",
    "    exist_ok=True,\n",
    "    plots=True,\n",
    "    save_period=5,\n",
    "    \n",
    "    name='train_kaggle',\n",
    "    project='runs/detect',\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 10. Salvar modelo final\n",
    "# =========================================================\n",
    "DEST_WEIGHTS = f\"{DRIVE_PROJECT}/weights_backup\"\n",
    "SOURCE_WEIGHTS = \"runs/detect/train_kaggle/weights\"\n",
    "\n",
    "os.makedirs(DEST_WEIGHTS, exist_ok=True)\n",
    "\n",
    "print(\"\\nüíæ Salvando modelo final...\")\n",
    "\n",
    "if os.path.exists(f\"{SOURCE_WEIGHTS}/best.pt\"):\n",
    "    shutil.copy(f\"{SOURCE_WEIGHTS}/best.pt\", f\"{DEST_WEIGHTS}/best_kaggle.pt\")\n",
    "    shutil.copy(f\"{SOURCE_WEIGHTS}/best.pt\", f\"{DRIVE_CHECKPOINTS}/best_final.pt\")\n",
    "    \n",
    "    with open(f\"{DEST_WEIGHTS}/class_mapping_kaggle.yaml\", \"w\") as f:\n",
    "        yaml.dump({\n",
    "            'simplified_names': SIMPLIFIED_NAMES,\n",
    "            'category_mapping': CATEGORY_MAPPING,\n",
    "            'original_classes': list(all_classes)\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"‚úÖ Modelo salvo: {DEST_WEIGHTS}/best_kaggle.pt\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è best.pt n√£o encontrado\")\n",
    "\n",
    "# Limpa checkpoints antigos (mant√©m apenas √∫ltimos 5)\n",
    "checkpoint_files = sorted(glob.glob(f\"{DRIVE_CHECKPOINTS}/epoch*.pt\"))\n",
    "if len(checkpoint_files) > 5:\n",
    "    for old_ckpt in checkpoint_files[:-5]:\n",
    "        os.remove(old_ckpt)\n",
    "        \n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nüìÅ Modelo final: {DEST_WEIGHTS}/best_kaggle.pt\")\n",
    "print(\"   Baixe esse arquivo e coloque em models/best.pt no seu PC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f3cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 11. VALIDA√á√ÉO DO MODELO TREINADO\n",
    "# =========================================================\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Configura√ß√£o\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/colab\"\n",
    "PROJECT_NAME = \"cloud-arch-security-mvp\"\n",
    "DRIVE_PROJECT = f\"{DRIVE_ROOT}/{PROJECT_NAME}\"\n",
    "DRIVE_CHECKPOINTS = f\"{DRIVE_PROJECT}/checkpoints\"\n",
    "\n",
    "# Monta Drive se necess√°rio\n",
    "from google.colab import drive\n",
    "if not os.path.exists('/content/drive/MyDrive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# Procura o modelo\n",
    "print(\"\\nüîç Procurando modelo treinado...\")\n",
    "\n",
    "model_paths = [\n",
    "    \"runs/detect/train_kaggle/weights/best.pt\",\n",
    "    f\"{DRIVE_CHECKPOINTS}/best.pt\",\n",
    "    f\"{DRIVE_CHECKPOINTS}/best_final.pt\",\n",
    "    f\"{DRIVE_PROJECT}/weights_backup/best_kaggle.pt\",\n",
    "]\n",
    "\n",
    "best_model_path = None\n",
    "for path in model_paths:\n",
    "    if os.path.exists(path):\n",
    "        best_model_path = path\n",
    "        print(f\"‚úÖ Modelo encontrado: {path}\")\n",
    "        break\n",
    "\n",
    "if not best_model_path:\n",
    "    print(\"‚ùå Nenhum modelo encontrado!\")\n",
    "else:\n",
    "    val_model = YOLO(best_model_path)\n",
    "    print(f\"üìä Modelo tem {len(val_model.names)} classes:\")\n",
    "    for idx, name in val_model.names.items():\n",
    "        print(f\"   {idx}: {name}\")\n",
    "    \n",
    "    # Valida√ß√£o\n",
    "    print(\"\\nüß™ Validando modelo...\")\n",
    "    \n",
    "    val_results = val_model.val(\n",
    "        data=\"dataset/data.yaml\",\n",
    "        split=\"test\",\n",
    "        plots=True,\n",
    "        save_json=True\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüìä M√âTRICAS DE VALIDA√á√ÉO:\")\n",
    "    print(f\"   mAP50: {val_results.box.map50:.4f}\")\n",
    "    print(f\"   mAP50-95: {val_results.box.map:.4f}\")\n",
    "    print(f\"   Precis√£o: {val_results.box.mp:.4f}\")\n",
    "    print(f\"   Recall: {val_results.box.mr:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìà mAP50 por categoria:\")\n",
    "    for i, name in enumerate(val_model.names.values()):\n",
    "        if i < len(val_results.box.ap50):\n",
    "            ap = val_results.box.ap50[i]\n",
    "            print(f\"   {name}: {ap:.4f}\")\n",
    "\n",
    "    # Teste visual\n",
    "    print(\"\\nüñºÔ∏è Testando em imagem de exemplo...\")\n",
    "    \n",
    "    test_images = list(Path(\"dataset/test/images\").glob(\"*.png\")) + \\\n",
    "                  list(Path(\"dataset/test/images\").glob(\"*.jpg\"))\n",
    "    \n",
    "    if test_images:\n",
    "        # Testa 3 imagens aleat√≥rias\n",
    "        for test_img in random.sample(test_images, min(3, len(test_images))):\n",
    "            print(f\"\\n   üì∑ {test_img.name}\")\n",
    "            \n",
    "            results = val_model(str(test_img), conf=0.25, verbose=False)\n",
    "            \n",
    "            detected = set()\n",
    "            for r in results:\n",
    "                for box in r.boxes:\n",
    "                    cls_name = val_model.names[int(box.cls[0])]\n",
    "                    conf = float(box.conf[0])\n",
    "                    detected.add(f\"{cls_name} ({conf:.2f})\")\n",
    "            \n",
    "            if detected:\n",
    "                print(f\"      Detectado: {', '.join(detected)}\")\n",
    "            else:\n",
    "                print(\"      ‚ö†Ô∏è Nenhuma detec√ß√£o\")\n",
    "            \n",
    "            # Exibe\n",
    "            result_img = results[0].plot()\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(result_img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Detec√ß√µes em {test_img.name}\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è Nenhuma imagem de teste encontrada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
